# üöÄ Credit Card Fraud Detection - Deployment Guide

## üìã **Complete Guide to Make Your Project Live**

---

## üéØ **Deployment Options Overview**

### **1. Web Application (Recommended)**
- **Streamlit** - Easy to deploy, great for demos
- **Flask/FastAPI** - More control, production-ready
- **Gradio** - Simple ML model deployment

### **2. Cloud Platforms**
- **Heroku** - Free tier, easy deployment
- **AWS** - Scalable, enterprise-grade
- **Google Cloud** - ML-focused services
- **Azure** - Microsoft ecosystem

### **3. API Services**
- **REST API** - For integration with other systems
- **Real-time API** - For live transaction processing

---

## üåê **Option 1: Streamlit Web App (Easiest)**

### **Step 1: Create Streamlit App**
```python
# app.py
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

def main():
    st.title("üí≥ Credit Card Fraud Detection")
    st.write("Upload transaction data or enter manually")
    
    # Add your model logic here
    # Add file upload
    # Add prediction interface

if __name__ == "__main__":
    main()
```

### **Step 2: Install Dependencies**
```bash
pip install streamlit pandas numpy scikit-learn imbalanced-learn plotly
```

### **Step 3: Run Locally**
```bash
streamlit run app.py
```

### **Step 4: Deploy to Streamlit Cloud**
1. Push code to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Connect your GitHub repo
4. Deploy automatically

---

## ‚òÅÔ∏è **Option 2: Heroku Deployment**

### **Step 1: Create Flask App**
```python
# app.py
from flask import Flask, request, jsonify
import pickle
import numpy as np

app = Flask(__name__)

# Load your trained model
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    features = np.array(data['features'])
    prediction = model.predict([features])[0]
    probability = model.predict_proba([features])[0]
    
    return jsonify({
        'prediction': int(prediction),
        'fraud_probability': float(probability[1]),
        'normal_probability': float(probability[0])
    })

if __name__ == '__main__':
    app.run(debug=True)
```

### **Step 2: Create Requirements File**
```txt
# requirements.txt
flask==2.0.1
pandas==1.3.0
numpy==1.21.0
scikit-learn==1.0.0
imbalanced-learn==0.8.0
gunicorn==20.1.0
```

### **Step 3: Create Procfile**
```txt
# Procfile
web: gunicorn app:app
```

### **Step 4: Deploy to Heroku**
```bash
# Install Heroku CLI
heroku create your-app-name
git add .
git commit -m "Initial deployment"
git push heroku main
```

---

## üê≥ **Option 3: Docker Container**

### **Step 1: Create Dockerfile**
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["gunicorn", "--bind", "0.0.0.0:5000", "app:app"]
```

### **Step 2: Build and Run**
```bash
docker build -t fraud-detection .
docker run -p 5000:5000 fraud-detection
```

---

## ‚òÅÔ∏è **Option 4: AWS Deployment**

### **Step 1: AWS Lambda Function**
```python
# lambda_function.py
import json
import pickle
import numpy as np

def lambda_handler(event, context):
    # Load model
    with open('model.pkl', 'rb') as f:
        model = pickle.load(f)
    
    # Get input data
    body = json.loads(event['body'])
    features = np.array(body['features'])
    
    # Make prediction
    prediction = model.predict([features])[0]
    probability = model.predict_proba([features])[0]
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'prediction': int(prediction),
            'fraud_probability': float(probability[1])
        })
    }
```

### **Step 2: Deploy to AWS**
1. Create AWS account
2. Install AWS CLI
3. Create Lambda function
4. Upload code and dependencies
5. Test the API

---

## üîß **Production-Ready Implementation**

### **Step 1: Model Serialization**
```python
# train_and_save_model.py
import pickle
import joblib
from sklearn.ensemble import RandomForestClassifier

# Train your model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save model
pickle.dump(model, open('model.pkl', 'wb'))
joblib.dump(model, 'model.joblib')
```

### **Step 2: API with Error Handling**
```python
# api.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import traceback

app = Flask(__name__)
CORS(app)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy'})

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        
        if not data or 'features' not in data:
            return jsonify({'error': 'Missing features'}), 400
        
        features = np.array(data['features'])
        
        # Validate input
        if len(features) != 30:
            return jsonify({'error': 'Expected 30 features'}), 400
        
        # Make prediction
        prediction = model.predict([features])[0]
        probability = model.predict_proba([features])[0]
        
        return jsonify({
            'prediction': int(prediction),
            'fraud_probability': float(probability[1]),
            'normal_probability': float(probability[0]),
            'confidence': float(max(probability))
        })
        
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': 'Internal server error'}), 500

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000)
```

---

## üìä **Real-Time Processing Setup**

### **Step 1: Message Queue (Redis/RabbitMQ)**
```python
# real_time_processor.py
import redis
import json
import numpy as np
from model import load_model

# Connect to Redis
redis_client = redis.Redis(host='localhost', port=6379, db=0)
model = load_model()

def process_transaction(transaction_data):
    """Process a single transaction"""
    features = extract_features(transaction_data)
    prediction = model.predict([features])[0]
    
    if prediction == 1:
        # Send alert
        send_fraud_alert(transaction_data)
    
    return prediction

def listen_for_transactions():
    """Listen for new transactions"""
    pubsub = redis_client.pubsub()
    pubsub.subscribe('transactions')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            transaction = json.loads(message['data'])
            process_transaction(transaction)
```

### **Step 2: Database Integration**
```python
# database.py
import sqlite3
import pandas as pd

class TransactionDB:
    def __init__(self, db_path='transactions.db'):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS transactions (
                id INTEGER PRIMARY KEY,
                amount REAL,
                time INTEGER,
                prediction INTEGER,
                fraud_probability REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.close()
    
    def save_transaction(self, amount, time, prediction, fraud_prob):
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            INSERT INTO transactions (amount, time, prediction, fraud_probability)
            VALUES (?, ?, ?, ?)
        ''', (amount, time, prediction, fraud_prob))
        conn.commit()
        conn.close()
```

---

## üîí **Security Considerations**

### **Step 1: API Authentication**
```python
# auth.py
from functools import wraps
from flask import request, jsonify
import jwt
import datetime

def require_auth(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = request.headers.get('Authorization')
        
        if not token:
            return jsonify({'error': 'Missing token'}), 401
        
        try:
            # Verify token
            payload = jwt.decode(token, 'your-secret-key', algorithms=['HS256'])
            request.user = payload
        except jwt.ExpiredSignatureError:
            return jsonify({'error': 'Token expired'}), 401
        except jwt.InvalidTokenError:
            return jsonify({'error': 'Invalid token'}), 401
        
        return f(*args, **kwargs)
    return decorated

@app.route('/predict', methods=['POST'])
@require_auth
def predict():
    # Your prediction logic here
    pass
```

### **Step 2: Input Validation**
```python
# validation.py
from marshmallow import Schema, fields, validate

class TransactionSchema(Schema):
    amount = fields.Float(required=True, validate=validate.Range(min=0))
    time = fields.Integer(required=True)
    v1 = fields.Float(required=True)
    v2 = fields.Float(required=True)
    # ... other fields

def validate_transaction(data):
    schema = TransactionSchema()
    try:
        validated_data = schema.load(data)
        return validated_data, None
    except ValidationError as e:
        return None, e.messages
```

---

## üìà **Monitoring and Analytics**

### **Step 1: Performance Monitoring**
```python
# monitoring.py
import time
import logging
from functools import wraps

def monitor_performance(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = f(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # Log performance metrics
            logging.info(f"Function {f.__name__} executed in {execution_time:.4f} seconds")
            
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logging.error(f"Function {f.__name__} failed after {execution_time:.4f} seconds: {str(e)}")
            raise
    
    return decorated

@app.route('/predict', methods=['POST'])
@monitor_performance
def predict():
    # Your prediction logic
    pass
```

### **Step 2: Analytics Dashboard**
```python
# analytics.py
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta

def create_analytics_dashboard():
    # Get data from database
    conn = sqlite3.connect('transactions.db')
    df = pd.read_sql_query("SELECT * FROM transactions", conn)
    conn.close()
    
    # Create visualizations
    fig1 = px.line(df, x='timestamp', y='amount', title='Transaction Amounts Over Time')
    fig2 = px.pie(df, names='prediction', title='Fraud vs Normal Transactions')
    
    return fig1, fig2
```

---

## üöÄ **Quick Start Guide**

### **For Beginners (Streamlit)**
1. **Install dependencies:**
   ```bash
   pip install streamlit pandas numpy scikit-learn
   ```

2. **Create app.py with the code above**

3. **Run locally:**
   ```bash
   streamlit run app.py
   ```

4. **Deploy to Streamlit Cloud:**
   - Push to GitHub
   - Connect to Streamlit Cloud
   - Deploy automatically

### **For Intermediate (Heroku)**
1. **Create Flask app**
2. **Add requirements.txt and Procfile**
3. **Deploy to Heroku:**
   ```bash
   heroku create your-app-name
   git push heroku main
   ```

### **For Advanced (AWS)**
1. **Create Lambda function**
2. **Set up API Gateway**
3. **Configure monitoring**
4. **Deploy and test**

---

## üìã **Deployment Checklist**

### **Pre-Deployment**
- [ ] Model is trained and saved
- [ ] Dependencies are listed in requirements.txt
- [ ] Error handling is implemented
- [ ] Input validation is added
- [ ] Security measures are in place

### **Deployment**
- [ ] Choose deployment platform
- [ ] Set up environment variables
- [ ] Configure database (if needed)
- [ ] Set up monitoring
- [ ] Test the deployment

### **Post-Deployment**
- [ ] Monitor performance
- [ ] Set up alerts
- [ ] Document API endpoints
- [ ] Create user documentation
- [ ] Plan for scaling

---

## üéØ **Recommended Approach**

### **For Portfolio/Demo:**
1. **Streamlit** - Easy, looks professional
2. **Heroku** - Free, good for demos

### **For Production:**
1. **AWS Lambda + API Gateway** - Scalable, cost-effective
2. **Docker + Cloud Platform** - Full control

### **For Learning:**
1. **Start with Streamlit** - Learn the basics
2. **Move to Flask** - Understand web frameworks
3. **Deploy to cloud** - Learn cloud services

---

**Your project is now ready to go live! Choose the option that best fits your needs and technical level.** üöÄ
